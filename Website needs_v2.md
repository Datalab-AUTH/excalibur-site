# EXCALIBUR WEBSITE
EXplainable and demoCrΑtized pipeLInes for unBiased, trUstworthy, and Responsible human-centric AI

**RED:** Suggested tabs (structured) \- buttons  
**BLACK:** Suggested text you can add as is in each tab. The text in “” can just be pasted, you can just modify how you present it.  
**ORANGE:** Notes/Sources used  
**BLUE:** Extra material  
\_\_\_

# TOP Aspects

* ### Home

  “The EXCALIBUR project is implemented under the Hellenic Foundation for Research and Innovation (HFRI / ELIDEK) through the call “Basic Research Financing (Horizontal support for all Sciences)”, which is part of Component 4.5 “Promoting Research and Innovation” of the National Recovery and Resilience Plan “Greece 2.0”, funded by the European Union – NextGenerationEU.

  In response to the growing need for **trustworthy and responsible artificial intelligence**, EXCALIBUR is structured around **three core components**:

* **Framework**

  A research-driven framework that leverages **Large Language Models (LLMs)** as advanced, **model-agnostic explainers and fairness evaluators**, integrating insights from established explainability and fairness approaches into **clear, human-understandable outputs**.

* **Toolset**

  An **open-source toolset and visualization platform** that embeds the framework into real AI pipelines and presents explanations and fairness reports in a **transparent, user-friendly way**, enabling human insight, interaction, and feedback.

* **Evaluation & Case Studies**

  A two-track validation approach combining **in-lab experimentation** for method development and fine-tuning with **in-the-wild case studies** involving stakeholders, assessing usability, trust, clarity, and overall impact through iterative evaluation.


  **The Key Pillars of EXCALIBUR**

* **Regulatory-to-technical mapping**

  The systematic translation of **Trustworthy AI principles and regulatory requirements** into concrete technical specifications and ethical guidelines that shape the framework and platform design.

* **Model-agnostic explainability and fairness, powered by LLMs**

  LLM-based components that produce **human-like explanations and contextual fairness interpretations** across tasks and metrics, supporting the identification and understanding of potential sources of bias in data and models.

* **Open tools and stakeholder-driven validation**

  An **open infrastructure** for reuse (covering code, tools, and platform components) combined with **continuous evaluation involving researchers and non-expert users**, ensuring practical usefulness and societal relevance.”

  Note: Source is [HFRI \- Hellenic Foundation for Research & Innovation](https://www.elidek.gr/en/2022/10/06/basic-research-financing-horizontal-support-for-all-sciences-national-recovery-and-resilience-plan-greece-2-0/) which is summarized.


* ### About

  * ### Objectives

#### Objective 1 \- Mapping Trustworthy AI Principles

**Title:** *Understand & Prioritize AI Ethics*  
**Description:** EXCALIBUR begins by systematically analyzing EU and global AI regulations and ethical frameworks, including the AI Act, AI Bill of Rights, and ACM principles. The project identifies and prioritizes human-centric requirements for transparent, fair, and accountable AI. The results are structured into a taxonomy that connects legal mandates, theoretical guidelines, and practical indicators, forming the foundation for responsible AI system design.  
**Impact:** Provides a clear roadmap for aligning AI development with ethical and legal standards.

#### Objective 2 \- LLM-based Explainability & Fairness

**Title:** *Design Transparent & Fair AI*  
**Description:** Building on Objective 1, EXCALIBUR integrates Large Language Models (LLMs) with state-of-the-art explainable AI (XAI) and fairness methods. The framework acts as a model-agnostic tool to deliver human-understandable explanations and fairness assessments for any AI system. This transforms black-box AI pipelines into transparent, trustworthy outputs that can be easily interpreted and validated by users, even non-experts.  
**Impact:** Enables a paradigm shift toward human-centric, trustworthy AI technologies.

#### Objective 3 \- Validation & Human-Centric Deployment

**Title:** *Test, Validate & Empower Users*  
**Description:** The EXCALIBUR framework is deployed in a human-centric domain (wearables) through an open-source platform that supports AI trustworthiness across the full ML pipeline. Through both “in-lab” and “in-the-wild” experiments, citizen-science approaches, and iterative feedback, the platform ensures human oversight, societal benefit, and actionable recommendations for responsible AI.  
**Impact:** Bridges technical innovation with real-world usability and societal impact.

* #### Methodological Approach

  “The methodological approach of EXCALIBUR is structured as an end-to-end strategy for **democratizing trustworthy and responsible AI**, by combining regulatory mapping, method development with Large Language Models (LLMs), tool building, and stakeholder-driven evaluation.  
  The project begins by **translating Trustworthy AI principles into actionable technical requirements**, with emphasis on **transparency, fairness, and accountability**, while also specifying **human-centric needs** (human oversight, societal well-being, safety) and establishing **ethical and data governance guidelines** to ensure GDPR-aligned experimentation.  
  Building on this foundation, EXCALIBUR designs a **model-agnostic LLM-based framework** with two core components: (i) an **explainability module** that learns from diverse XAI methods (e.g., LIME-like to more advanced techniques) to generate human-readable explanations, and (ii) a **fairness evaluation module** that computes and interprets a broad set of fairness metrics and uses the LLM to explain their meaning in context and help uncover potential sources of bias in data and models.  
  The framework is then validated through **two complementary experimentation tracks** in the wearable domain: **in-lab evaluation** using baseline AI models built on public wearable datasets, and **in-the-wild case studies** that involve non-expert stakeholders in iterative rounds to assess usability, trust, clarity of explanations, and usefulness of fairness reports. This process follows a citizen-science-inspired approach to ensure continuous feedback and real-world relevance.  
  Finally, EXCALIBUR delivers an **open-source toolset and visualization platform** (e.g., a Python library plus a web-based interface) that integrates the framework into real AI pipelines, presents results in a human-friendly way, supports user intervention and feedback, and applies standard security and privacy practices for safe deployment.”  
  Source: This is basically a summary of the Methodology of part B2.1: [Final submission files \- Google Drive](https://drive.google.com/drive/folders/1r5Qohwpdao6AE8W8lfoHdG8y2wR_mQsD)

* ### Research & Outputs

  * #### Publications

  * #### Software & Tools

    Note: Needed because of WP4: “Open-source **toolset**/platform”, Deliverables D4.1, D4.2, plus GitHub explicitly mentioned in WP1

* #### Citizen Science / Case Studies

  We need it because of WP5, since we mention Explicit Citizen Science (CS) approach \+ in-the-wild experimentation

We will include: Description of in-lab vs in-the-wild studies, Stakeholder involvement, **Possibly a “Get involved” call later**

\_\_\_

# BOTTOM Aspects

- ##### Contact

  \~etc (up to you)  
- **At the bottom of the page you can add:**  
  “This project is funded by the General Secretariat for Research and Technology (GSRT) and the Hellenic Foundation for Research and Innovation (HFRI), under the National Recovery and Resilience Plan “Greece 2.0”, funded by the European Union – NextGenerationEU.”

## Extra Material

### **Useful links (of ELIDEK)** [Main ΕΛΙΔΕΚ website](https://www.elidek.gr/en/homepage/)

[Facebook](https://www.facebook.com/ELIDEKgr/)

[Twitter](https://x.com/ELIDEK_HFRI)

[Linkedin](https://www.linkedin.com/company/hellenic-foundation-for-research-and-innovation/)

### **Logos**

[**logos \- Google Drive**](https://drive.google.com/drive/folders/1Hm-1VteRols8z6dHVmErpfIVWbmPwETo)

